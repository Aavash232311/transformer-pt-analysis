{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f21994c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c886bde",
   "metadata": {},
   "source": [
    "### Loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e01518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2, 2, 4, 6, 0, 6, 6, 2, 8]), tensor([2, 4, 6, 0, 6, 6, 2, 8, 0]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.mintrans import FibonacciModDataset, MinimalTransformer, evaluate_model, train_model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = FibonacciModDataset(num_samples=10)\n",
    "print(data.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c1621",
   "metadata": {},
   "source": [
    "### With the default `10` epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e462b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2085\n",
      "Epoch 2, Loss: 1.3469\n",
      "Epoch 3, Loss: 0.9227\n",
      "Epoch 4, Loss: 0.7797\n",
      "Epoch 5, Loss: 0.6970\n",
      "Epoch 6, Loss: 0.6500\n",
      "Epoch 7, Loss: 0.6144\n",
      "Epoch 8, Loss: 0.5838\n",
      "Epoch 9, Loss: 0.5582\n",
      "Epoch 10, Loss: 0.5343\n",
      "Epoch 11, Loss: 0.5168\n",
      "Epoch 12, Loss: 0.5009\n",
      "Total Training Time: 9.10 seconds\n",
      "Accuracy: 83.65%\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "train_ds = FibonacciModDataset(num_samples=5000, mod=vocab_size)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "model = MinimalTransformer(vocab_size=vocab_size).to(device=device)\n",
    "train_model(model, train_loader)\n",
    "evaluate_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694928d0",
   "metadata": {},
   "source": [
    "### Epoch increased to `100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "131a23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4848\n",
      "Epoch 2, Loss: 0.4731\n",
      "Epoch 3, Loss: 0.4632\n",
      "Epoch 4, Loss: 0.4517\n",
      "Epoch 5, Loss: 0.4482\n",
      "Epoch 6, Loss: 0.4443\n",
      "Epoch 7, Loss: 0.4356\n",
      "Epoch 8, Loss: 0.4338\n",
      "Epoch 9, Loss: 0.4325\n",
      "Epoch 10, Loss: 0.4235\n",
      "Epoch 11, Loss: 0.4285\n",
      "Epoch 12, Loss: 0.4201\n",
      "Epoch 13, Loss: 0.4167\n",
      "Epoch 14, Loss: 0.4155\n",
      "Epoch 15, Loss: 0.4127\n",
      "Epoch 16, Loss: 0.4070\n",
      "Epoch 17, Loss: 0.4126\n",
      "Epoch 18, Loss: 0.4066\n",
      "Epoch 19, Loss: 0.4104\n",
      "Epoch 20, Loss: 0.4030\n",
      "Epoch 21, Loss: 0.3977\n",
      "Epoch 22, Loss: 0.3975\n",
      "Epoch 23, Loss: 0.3959\n",
      "Epoch 24, Loss: 0.4166\n",
      "Epoch 25, Loss: 0.3956\n",
      "Epoch 26, Loss: 0.4029\n",
      "Epoch 27, Loss: 0.4009\n",
      "Epoch 28, Loss: 0.3895\n",
      "Epoch 29, Loss: 0.3946\n",
      "Epoch 30, Loss: 0.3896\n",
      "Epoch 31, Loss: 0.3911\n",
      "Epoch 32, Loss: 0.4027\n",
      "Epoch 33, Loss: 0.3889\n",
      "Epoch 34, Loss: 0.3887\n",
      "Epoch 35, Loss: 0.3889\n",
      "Epoch 36, Loss: 0.3948\n",
      "Epoch 37, Loss: 0.3885\n",
      "Epoch 38, Loss: 0.3938\n",
      "Epoch 39, Loss: 0.3797\n",
      "Epoch 40, Loss: 0.3901\n",
      "Total Training Time: 29.22 seconds\n",
      "Accuracy: 86.44%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, epochs=40)\n",
    "evaluate_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aecadb",
   "metadata": {},
   "source": [
    "We have `~5.42%` accuracy increase with `10` times more epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce2a1f",
   "metadata": {},
   "source": [
    "### Increasing the batch size from `32` to `64` with epoch as `10`\n",
    "\n",
    "Accuracy goes from ~75-80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14714fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2940\n",
      "Epoch 2, Loss: 1.8292\n",
      "Epoch 3, Loss: 1.5082\n",
      "Epoch 4, Loss: 1.3372\n",
      "Epoch 5, Loss: 1.2004\n",
      "Epoch 6, Loss: 1.0723\n",
      "Epoch 7, Loss: 0.9414\n",
      "Epoch 8, Loss: 0.8482\n",
      "Epoch 9, Loss: 0.7849\n",
      "Epoch 10, Loss: 0.7374\n",
      "Epoch 11, Loss: 0.6887\n",
      "Epoch 12, Loss: 0.6579\n",
      "Total Training Time: 2.89 seconds\n",
      "Accuracy: 78.31%\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10 # that is mod in our case\n",
    "train_ds = FibonacciModDataset(num_samples=5000, mod=vocab_size)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "model = MinimalTransformer(vocab_size=vocab_size).to(device=device)\n",
    "train_model(model, train_loader)\n",
    "evaluate_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3d209",
   "metadata": {},
   "source": [
    "### Switching back to default `batch_size` of `32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "706dbe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0016\n",
      "Epoch 2, Loss: 1.1204\n",
      "Epoch 3, Loss: 0.9242\n",
      "Epoch 4, Loss: 0.8132\n",
      "Epoch 5, Loss: 0.7289\n",
      "Epoch 6, Loss: 0.6728\n",
      "Epoch 7, Loss: 0.6383\n",
      "Epoch 8, Loss: 0.6069\n",
      "Epoch 9, Loss: 0.5875\n",
      "Epoch 10, Loss: 0.5706\n",
      "Epoch 11, Loss: 0.5528\n",
      "Epoch 12, Loss: 0.5378\n",
      "Total Training Time: 5.56 seconds\n",
      "Accuracy: 81.48%\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10 # that is mod in our case\n",
    "train_ds = FibonacciModDataset(num_samples=5000, mod=vocab_size)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "model = MinimalTransformer(vocab_size=vocab_size).to(device=device)\n",
    "train_model(model, train_loader)\n",
    "evaluate_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457b8a2",
   "metadata": {},
   "source": [
    "## Splitted dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91f4275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2056\n",
      "Epoch 2, Loss: 1.4975\n",
      "Epoch 3, Loss: 1.0486\n",
      "Epoch 4, Loss: 0.8584\n",
      "Epoch 5, Loss: 0.7431\n",
      "Epoch 6, Loss: 0.6609\n",
      "Epoch 7, Loss: 0.5904\n",
      "Epoch 8, Loss: 0.5428\n",
      "Epoch 9, Loss: 0.5086\n",
      "Epoch 10, Loss: 0.4825\n",
      "Epoch 11, Loss: 0.4651\n",
      "Epoch 12, Loss: 0.4503\n",
      "Total Training Time: 7.67 seconds\n",
      "Accuracy: 84.91%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "vocab_size = 10\n",
    "generated_ds = FibonacciModDataset(num_samples=5000, mod=vocab_size)\n",
    "train_size = int(0.8 * len(generated_ds)) # 80% to train\n",
    "test_size = len(generated_ds) - train_size # rest of the size\n",
    "\n",
    "train_ds, test_ds = random_split(generated_ds, [train_size, test_size]) # randomly splits our dataset\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "model = MinimalTransformer(vocab_size=vocab_size).to(device=device)\n",
    "train_model(model, train_loader)\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c598634",
   "metadata": {},
   "source": [
    "## Splitted dataset and increase in number of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30c5297f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4396\n",
      "Epoch 2, Loss: 0.4281\n",
      "Epoch 3, Loss: 0.4209\n",
      "Epoch 4, Loss: 0.4162\n",
      "Epoch 5, Loss: 0.4101\n",
      "Epoch 6, Loss: 0.4073\n",
      "Epoch 7, Loss: 0.4027\n",
      "Epoch 8, Loss: 0.3937\n",
      "Epoch 9, Loss: 0.3876\n",
      "Epoch 10, Loss: 0.3877\n",
      "Epoch 11, Loss: 0.3894\n",
      "Epoch 12, Loss: 0.3961\n",
      "Epoch 13, Loss: 0.3793\n",
      "Epoch 14, Loss: 0.3711\n",
      "Epoch 15, Loss: 0.3725\n",
      "Epoch 16, Loss: 0.3714\n",
      "Epoch 17, Loss: 0.3675\n",
      "Epoch 18, Loss: 0.3629\n",
      "Epoch 19, Loss: 0.3669\n",
      "Epoch 20, Loss: 0.3947\n",
      "Epoch 21, Loss: 0.3614\n",
      "Epoch 22, Loss: 0.3645\n",
      "Epoch 23, Loss: 0.3609\n",
      "Epoch 24, Loss: 0.3557\n",
      "Epoch 25, Loss: 0.3534\n",
      "Epoch 26, Loss: 0.3624\n",
      "Epoch 27, Loss: 0.3521\n",
      "Epoch 28, Loss: 0.3499\n",
      "Epoch 29, Loss: 0.3490\n",
      "Epoch 30, Loss: 0.3495\n",
      "Epoch 31, Loss: 0.3479\n",
      "Epoch 32, Loss: 0.3565\n",
      "Epoch 33, Loss: 0.3511\n",
      "Epoch 34, Loss: 0.3419\n",
      "Epoch 35, Loss: 0.3448\n",
      "Epoch 36, Loss: 0.3447\n",
      "Epoch 37, Loss: 0.3472\n",
      "Epoch 38, Loss: 0.3524\n",
      "Epoch 39, Loss: 0.3426\n",
      "Epoch 40, Loss: 0.3446\n",
      "Epoch 41, Loss: 0.3423\n",
      "Epoch 42, Loss: 0.3372\n",
      "Epoch 43, Loss: 0.3382\n",
      "Epoch 44, Loss: 0.3374\n",
      "Epoch 45, Loss: 0.3364\n",
      "Epoch 46, Loss: 0.3808\n",
      "Epoch 47, Loss: 0.3394\n",
      "Epoch 48, Loss: 0.3340\n",
      "Epoch 49, Loss: 0.3353\n",
      "Epoch 50, Loss: 0.3359\n",
      "Epoch 51, Loss: 0.3305\n",
      "Epoch 52, Loss: 0.3338\n",
      "Epoch 53, Loss: 0.3349\n",
      "Epoch 54, Loss: 0.3351\n",
      "Epoch 55, Loss: 0.3322\n",
      "Epoch 56, Loss: 0.3439\n",
      "Epoch 57, Loss: 0.3447\n",
      "Epoch 58, Loss: 0.3294\n",
      "Epoch 59, Loss: 0.3360\n",
      "Epoch 60, Loss: 0.3283\n",
      "Epoch 61, Loss: 0.3348\n",
      "Epoch 62, Loss: 0.3281\n",
      "Epoch 63, Loss: 0.3279\n",
      "Epoch 64, Loss: 0.3293\n",
      "Epoch 65, Loss: 0.3510\n",
      "Epoch 66, Loss: 0.3275\n",
      "Epoch 67, Loss: 0.3280\n",
      "Epoch 68, Loss: 0.3281\n",
      "Epoch 69, Loss: 0.3277\n",
      "Epoch 70, Loss: 0.3339\n",
      "Epoch 71, Loss: 0.3371\n",
      "Epoch 72, Loss: 0.3550\n",
      "Epoch 73, Loss: 0.3318\n",
      "Epoch 74, Loss: 0.3262\n",
      "Epoch 75, Loss: 0.3240\n",
      "Epoch 76, Loss: 0.3412\n",
      "Epoch 77, Loss: 0.3281\n",
      "Epoch 78, Loss: 0.3389\n",
      "Epoch 79, Loss: 0.3249\n",
      "Epoch 80, Loss: 0.3251\n",
      "Epoch 81, Loss: 0.3235\n",
      "Epoch 82, Loss: 0.3284\n",
      "Epoch 83, Loss: 0.3262\n",
      "Epoch 84, Loss: 0.3256\n",
      "Epoch 85, Loss: 0.3237\n",
      "Epoch 86, Loss: 0.3336\n",
      "Epoch 87, Loss: 0.3239\n",
      "Epoch 88, Loss: 0.3239\n",
      "Epoch 89, Loss: 0.3365\n",
      "Epoch 90, Loss: 0.3471\n",
      "Epoch 91, Loss: 0.3241\n",
      "Epoch 92, Loss: 0.3256\n",
      "Epoch 93, Loss: 0.3261\n",
      "Epoch 94, Loss: 0.3239\n",
      "Epoch 95, Loss: 0.3360\n",
      "Epoch 96, Loss: 0.3553\n",
      "Epoch 97, Loss: 0.3247\n",
      "Epoch 98, Loss: 0.3226\n",
      "Epoch 99, Loss: 0.3209\n",
      "Epoch 100, Loss: 0.3223\n",
      "Epoch 101, Loss: 0.3235\n",
      "Epoch 102, Loss: 0.3338\n",
      "Epoch 103, Loss: 0.3214\n",
      "Epoch 104, Loss: 0.3212\n",
      "Epoch 105, Loss: 0.3203\n",
      "Epoch 106, Loss: 0.3232\n",
      "Epoch 107, Loss: 0.3223\n",
      "Epoch 108, Loss: 0.3191\n",
      "Epoch 109, Loss: 0.3241\n",
      "Epoch 110, Loss: 0.3605\n",
      "Epoch 111, Loss: 0.3245\n",
      "Epoch 112, Loss: 0.3221\n",
      "Epoch 113, Loss: 0.3188\n",
      "Epoch 114, Loss: 0.3273\n",
      "Epoch 115, Loss: 0.3214\n",
      "Epoch 116, Loss: 0.3261\n",
      "Epoch 117, Loss: 0.3177\n",
      "Epoch 118, Loss: 0.3194\n",
      "Epoch 119, Loss: 0.3185\n",
      "Epoch 120, Loss: 0.3185\n",
      "Epoch 121, Loss: 0.3181\n",
      "Epoch 122, Loss: 0.3206\n",
      "Epoch 123, Loss: 0.3167\n",
      "Epoch 124, Loss: 0.3504\n",
      "Epoch 125, Loss: 0.3365\n",
      "Epoch 126, Loss: 0.3185\n",
      "Epoch 127, Loss: 0.3174\n",
      "Epoch 128, Loss: 0.3156\n",
      "Epoch 129, Loss: 0.3152\n",
      "Epoch 130, Loss: 0.3147\n",
      "Epoch 131, Loss: 0.3234\n",
      "Epoch 132, Loss: 0.3260\n",
      "Epoch 133, Loss: 0.3160\n",
      "Epoch 134, Loss: 0.3185\n",
      "Epoch 135, Loss: 0.3185\n",
      "Epoch 136, Loss: 0.3184\n",
      "Epoch 137, Loss: 0.3137\n",
      "Epoch 138, Loss: 0.3189\n",
      "Epoch 139, Loss: 0.3144\n",
      "Epoch 140, Loss: 0.3549\n",
      "Epoch 141, Loss: 0.3310\n",
      "Epoch 142, Loss: 0.3164\n",
      "Epoch 143, Loss: 0.3183\n",
      "Epoch 144, Loss: 0.3138\n",
      "Epoch 145, Loss: 0.3158\n",
      "Epoch 146, Loss: 0.3309\n",
      "Epoch 147, Loss: 0.3191\n",
      "Epoch 148, Loss: 0.3164\n",
      "Epoch 149, Loss: 0.3146\n",
      "Epoch 150, Loss: 0.3196\n",
      "Mean accuracy across epochs (Training): 88.0565%\n",
      "Accuracy: 88.48%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "def train_model(model, dataloader, epochs=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    accuracy_per_e = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc = correct / total\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy_per_e.append(acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Mean accuracy across epochs (Training): {sum(accuracy_per_e) / len(accuracy_per_e):.4%}\")\n",
    "\n",
    "train_model(model, train_loader, epochs=150)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca2c2d",
   "metadata": {},
   "source": [
    "## Hyperparamater adjustment\n",
    "\n",
    "Adjusting hypermaters:- (model no longer learns)\n",
    "\n",
    "\n",
    "`batch_size`: `256`\n",
    "\n",
    "`epoch`: `20`\n",
    "\n",
    "`LR`: `10^-3`\n",
    "\n",
    "`d_model`: `32`\n",
    "\n",
    "`n_head`: `8`\n",
    "\n",
    "`n_layer`: `6` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c380c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.7638\n",
      "Epoch 2, Loss: 1.8719\n",
      "Epoch 3, Loss: 1.1850\n",
      "Epoch 4, Loss: 0.7841\n",
      "Epoch 5, Loss: 0.5680\n",
      "Epoch 6, Loss: 0.4283\n",
      "Epoch 7, Loss: 0.3474\n",
      "Epoch 8, Loss: 0.3066\n",
      "Epoch 9, Loss: 0.2869\n",
      "Epoch 10, Loss: 0.2835\n",
      "Epoch 11, Loss: 0.2811\n",
      "Epoch 12, Loss: 0.3027\n",
      "Epoch 13, Loss: 0.3268\n",
      "Epoch 14, Loss: 0.3183\n",
      "Epoch 15, Loss: 0.3314\n",
      "Epoch 16, Loss: 0.3211\n",
      "Epoch 17, Loss: 0.3060\n",
      "Epoch 18, Loss: 0.3006\n",
      "Epoch 19, Loss: 0.2930\n",
      "Epoch 20, Loss: 0.2812\n",
      "Epoch 21, Loss: 0.2802\n",
      "Epoch 22, Loss: 0.2741\n",
      "Epoch 23, Loss: 0.2739\n",
      "Epoch 24, Loss: 0.2709\n",
      "Epoch 25, Loss: 0.2697\n",
      "Epoch 26, Loss: 0.2689\n",
      "Epoch 27, Loss: 0.2649\n",
      "Epoch 28, Loss: 0.2640\n",
      "Epoch 29, Loss: 0.2667\n",
      "Epoch 30, Loss: 0.2674\n",
      "Epoch 31, Loss: 0.2657\n",
      "Epoch 32, Loss: 0.2685\n",
      "Epoch 33, Loss: 0.2680\n",
      "Epoch 34, Loss: 0.2682\n",
      "Epoch 35, Loss: 0.2666\n",
      "Epoch 36, Loss: 0.2650\n",
      "Epoch 37, Loss: 0.2663\n",
      "Epoch 38, Loss: 0.2652\n",
      "Epoch 39, Loss: 0.2643\n",
      "Epoch 40, Loss: 0.2623\n",
      "Epoch 41, Loss: 0.2624\n",
      "Epoch 42, Loss: 0.2609\n",
      "Epoch 43, Loss: 0.2627\n",
      "Epoch 44, Loss: 0.2642\n",
      "Epoch 45, Loss: 0.2634\n",
      "Epoch 46, Loss: 0.2629\n",
      "Epoch 47, Loss: 0.2637\n",
      "Epoch 48, Loss: 0.2643\n",
      "Epoch 49, Loss: 0.2642\n",
      "Epoch 50, Loss: 0.2643\n",
      "Epoch 51, Loss: 0.2643\n",
      "Epoch 52, Loss: 0.2637\n",
      "Epoch 53, Loss: 0.2638\n",
      "Epoch 54, Loss: 0.2640\n",
      "Epoch 55, Loss: 0.2633\n",
      "Epoch 56, Loss: 0.2652\n",
      "Mean accuracy across epochs (Training): 86.9094%\n",
      "Accuracy: 90.10%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocab_size = 10\n",
    "batch_size = 1536\n",
    "generated_ds = FibonacciModDataset(num_samples=5000, mod=vocab_size)\n",
    "\n",
    "train_size = int(0.8 * len(generated_ds))\n",
    "test_size = len(generated_ds) - train_size\n",
    "train_ds, test_ds = random_split(generated_ds, [train_size, test_size]) \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = MinimalTransformer(vocab_size=vocab_size, d_model=512,n_heads=16, num_layers=1).to(device)\n",
    "\n",
    "train_model(model, train_loader, epochs=56)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f83b8",
   "metadata": {},
   "source": [
    "## Change in Fib Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4eeb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1516\n",
      "Epoch 2, Loss: 1.4913\n",
      "Epoch 3, Loss: 0.9155\n",
      "Epoch 4, Loss: 0.7043\n",
      "Epoch 5, Loss: 0.5824\n",
      "Epoch 6, Loss: 0.4974\n",
      "Epoch 7, Loss: 0.4328\n",
      "Epoch 8, Loss: 0.3848\n",
      "Epoch 9, Loss: 0.3515\n",
      "Epoch 10, Loss: 0.3262\n",
      "Epoch 11, Loss: 0.3076\n",
      "Epoch 12, Loss: 0.2928\n",
      "Epoch 13, Loss: 0.2821\n",
      "Epoch 14, Loss: 0.2727\n",
      "Epoch 15, Loss: 0.2648\n",
      "Epoch 16, Loss: 0.2573\n",
      "Epoch 17, Loss: 0.2517\n",
      "Epoch 18, Loss: 0.2483\n",
      "Epoch 19, Loss: 0.2439\n",
      "Epoch 20, Loss: 0.2394\n",
      "Epoch 21, Loss: 0.2363\n",
      "Epoch 22, Loss: 0.2316\n",
      "Epoch 23, Loss: 0.2269\n",
      "Epoch 24, Loss: 0.2222\n",
      "Epoch 25, Loss: 0.2185\n",
      "Epoch 26, Loss: 0.2171\n",
      "Epoch 27, Loss: 0.2162\n",
      "Epoch 28, Loss: 0.2146\n",
      "Epoch 29, Loss: 0.2118\n",
      "Epoch 30, Loss: 0.2122\n",
      "Total Training Time: 6.78 seconds\n",
      "Accuracy: 91.96%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class FibonacciModDataset(Dataset):\n",
    "    def __init__(self, seq_len=10, mod=10, num_samples=10000):\n",
    "        self.mod = mod\n",
    "\n",
    "        self.global_seq = self.generate_fib_sequence(1000, mod)\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            start_idx = torch.randint(0, len(self.global_seq) - seq_len - 1, (1,)).item()\n",
    "            seq = self.global_seq[start_idx:start_idx + seq_len + 1]\n",
    "            x = torch.tensor(seq[:-1], dtype=torch.long)\n",
    "            y = torch.tensor(seq[1:], dtype=torch.long)\n",
    "            self.samples.append((x, y))\n",
    "\n",
    "    def generate_fib_sequence(self, length, mod):\n",
    "        seq = [1, 1] # these are the starting values\n",
    "        while len(seq) < length: # this prevents overlap\n",
    "            seq.append((seq[-1] + seq[-2]) % mod)\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "class MinimalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, n_heads=2, num_layers=1, max_seq_len=20):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        B, T = tokens.shape\n",
    "        pos = torch.arange(T, device=tokens.device)\n",
    "        x = self.token_embed(tokens) + self.pos_embed(pos).unsqueeze(0)\n",
    "        attn_mask = torch.triu(torch.ones(T, T, device=tokens.device) * float('-inf'), diagonal=1)\n",
    "        for attn in self.layers:\n",
    "            attn_out, _ = attn(x, x, x, attn_mask=attn_mask)\n",
    "            x = x + attn_out\n",
    "        return self.out_proj(x)\n",
    "\n",
    "def train_model(model, dataloader, epochs=12, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    print(f\"Accuracy: {correct / total:.2%}\")\n",
    "\n",
    "vocab_size = 10\n",
    "batch_size = 128\n",
    "generated_ds = FibonacciModDataset(num_samples=5000, mod=vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(generated_ds))\n",
    "test_size = len(generated_ds) - train_size\n",
    "train_ds, test_ds = random_split(generated_ds, [train_size, test_size]) \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = MinimalTransformer(vocab_size=vocab_size).to(device)\n",
    "\n",
    "train_model(model, train_loader, epochs=30)\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
